{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Activation, Dense, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Masking, TimeDistributed, Lambda\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import SGD, Adam\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mask_utilities import ZeroMaskedEntries, mask_aware_max, mask_aware_mean, mask_aware_mean_output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mil_dataset(bag_sizes, prob_inst_positive=None, random_state=123456, **make_classification_kwargs):\n",
    "    \"\"\"This is not going to be efficient\"\"\"\n",
    "    if prob_inst_positive is None:\n",
    "        prob_inst_positive = 1 - np.exp(np.log(0.5)/np.mean(bag_sizes))\n",
    "        \n",
    "    X, y= make_classification(n_samples=np.asarray(bag_sizes).sum()*3, random_state=random_state)\n",
    "    negative = X[y==0]\n",
    "    positive = X[y==1]\n",
    "    neg_pos = 0\n",
    "    pos_pos = 0\n",
    "    bags = []\n",
    "    labels = []\n",
    "    for i in range(len(bag_sizes)):\n",
    "        bagdata = []\n",
    "        baglabels = (np.random.uniform(size=bag_sizes[i]) > (1-prob_inst_positive)).astype(int)\n",
    "        for lab in baglabels:\n",
    "            if lab == 0:\n",
    "                bagdata.append(negative[neg_pos])\n",
    "                neg_pos += 1\n",
    "            else:\n",
    "                bagdata.append(positive[pos_pos])\n",
    "                pos_pos += 1\n",
    "        bagdata = np.array(bagdata)\n",
    "#         print(bagdata.shape)\n",
    "#         print(baglabels.shape)\n",
    "        bags.append(np.hstack([bagdata, baglabels.reshape(-1, 1)]))\n",
    "        labels.append(max(baglabels))\n",
    "    bags = np.array(bags)\n",
    "    labels = np.array(labels)\n",
    "    return bags,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ array([[-0.02746227,  0.24064694,  0.34312528,  0.45242869,  2.13782675,\n",
       "         0.48310266,  0.87783281, -0.93118368,  0.32410174, -1.23386202,\n",
       "        -0.58196715, -0.65841844,  1.23327337, -3.52087607, -1.40304778,\n",
       "         0.51686997,  0.37521779, -0.0614506 , -0.21051828,  0.03311313,\n",
       "         1.        ]]),\n",
       "       array([[ 2.12145332,  0.60460316,  0.37795332, -2.46146711,  0.49367226,\n",
       "        -1.55390218,  0.79549483,  1.48734885,  0.96766064, -0.92879728,\n",
       "         1.3750197 ,  0.56369984, -0.74320592, -0.30885258, -1.05790875,\n",
       "        -0.68108726, -1.4331399 ,  0.87556247,  1.14599999,  0.59770053,\n",
       "         1.        ],\n",
       "       [-0.31744079,  3.35742705,  1.51997029,  0.60017759, -0.49366211,\n",
       "         0.27422987, -0.92999096,  0.01487095, -0.48760159,  0.38039557,\n",
       "        -2.18293672,  0.89617065, -1.96978545,  0.08484421, -0.08224033,\n",
       "         0.43238977, -1.02951028,  0.39702147,  2.39678042, -1.23626892,\n",
       "         0.        ]]),\n",
       "       array([[ 0.44115327, -1.07135667, -1.34089625, -1.32886541,  1.84688341,\n",
       "         1.68270572, -1.74907225,  0.78183575,  0.22147063,  1.72968859,\n",
       "         0.75852711,  0.58378655,  0.09461058, -0.96498   , -0.74447138,\n",
       "        -0.8456957 ,  1.68408972, -1.15467063, -1.28278174,  2.35392486,\n",
       "         0.        ],\n",
       "       [-0.56085934, -0.75851402, -0.66804914,  0.31342068,  0.24250114,\n",
       "         0.12092999, -1.04110844, -1.6240619 , -1.68110141,  0.38793581,\n",
       "         0.60986195, -1.07067775,  1.07712214,  1.8150662 ,  0.40377646,\n",
       "         0.70519992,  1.97515803, -1.19816859, -0.9173684 , -0.58900454,\n",
       "         0.        ],\n",
       "       [ 1.95254106, -0.00843352, -1.65249889, -0.89648411,  1.48875344,\n",
       "         0.57689722,  1.30233254,  0.37931942, -1.22697005, -0.23009554,\n",
       "        -0.50751604,  0.53394577, -1.35379156,  0.39449999,  0.04040271,\n",
       "        -1.93437003, -2.47685142,  1.50200808,  0.66108393, -1.05665208,\n",
       "         1.        ]]),\n",
       "       array([[ 0.40330952,  0.17644434, -0.34535219,  0.69057934,  1.31423238,\n",
       "         0.9957609 , -0.11462759,  0.68773839, -2.17986061,  1.46269605,\n",
       "        -0.95420784,  0.30162445, -0.73566162, -1.74316091, -1.36984936,\n",
       "        -0.82659092, -0.59655349,  0.2955964 , -0.73233937, -0.15495077,\n",
       "         0.        ],\n",
       "       [-0.73977634,  0.74408632, -0.74134265, -0.9228752 , -0.07967307,\n",
       "         0.30363782, -0.87073597,  0.9923118 ,  0.63958879,  0.10404981,\n",
       "         1.90668447, -0.17964175,  1.20207244,  0.17406769,  1.58501403,\n",
       "        -0.43946096,  1.93896255, -1.15283939,  0.38805436, -1.05487432,\n",
       "         0.        ],\n",
       "       [-0.25506942,  0.18473498,  0.62555464,  0.24969795, -0.05926791,\n",
       "         1.10394893, -2.1883451 ,  0.03621956,  0.29463311, -0.68559748,\n",
       "         0.84697374,  1.28839346,  0.60630771,  0.60909891, -1.1657872 ,\n",
       "        -0.30396116,  2.57199805, -1.6888522 ,  0.26213581, -0.27101967,\n",
       "         0.        ],\n",
       "       [-2.41731194,  0.56363651, -1.15064062, -0.5576966 , -0.79833414,\n",
       "         0.38135311, -0.11259151, -1.2568604 ,  1.12965899, -0.21750304,\n",
       "        -0.44564485,  0.04129319,  1.89877782, -1.42036061,  0.08692636,\n",
       "        -0.01560077,  1.9119556 , -1.02153506,  1.14786216,  0.97282687,\n",
       "         0.        ]])], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, labels = create_mil_dataset(np.array([1,2,3,4]))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47999999999999998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(create_mil_dataset(np.array([6]*200))[1])  # balancing of classes is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, labels = create_mil_dataset(np.random.randint(5, 20, size=600))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_sil_dataset(mil_dataset):\n",
    "    sil_dataset = None\n",
    "    for bag in dataset:\n",
    "        if sil_dataset is None:\n",
    "            sil_dataset = bag\n",
    "        sil_dataset = np.vstack([sil_dataset, bag])\n",
    "    return sil_dataset[:, :-1], sil_dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7249, 20)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sil, Y_sil = extract_sil_dataset(dataset)\n",
    "X_sil.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfeats = dataset[0][0].shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inlayer = Input([None, nfeats])\n",
    "masked_input = Masking()(inlayer)\n",
    "layer2 = TimeDistributed(Dense(30, activation='relu'))(masked_input)\n",
    "layer3 = TimeDistributed(Dense(1, activation='relu'))(layer2)\n",
    "instance_predictions = Activation('sigmoid')(layer3)\n",
    "zeroed_layer3 = ZeroMaskedEntries()(instance_predictions)\n",
    "layer4 = Lambda(mask_aware_max, mask_aware_mean_output_shape)(zeroed_layer3)\n",
    "model = Model([inlayer], [layer4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, None, 20),\n",
       " (None, None, 20),\n",
       " (None, None, 30),\n",
       " (None, None, 1),\n",
       " (None, None, 1),\n",
       " (None, None, 1),\n",
       " (None, 1)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l.output_shape for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x206648b1668>,\n",
       " <keras.layers.core.Masking at 0x206648b16a0>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x206648b1b38>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x206648d4ac8>,\n",
       " <keras.layers.core.Activation at 0x206648b16d8>,\n",
       " <mask_utilities.ZeroMaskedEntries at 0x206648d4b00>,\n",
       " <keras.layers.core.Lambda at 0x206648ad4e0>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83121371]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([dataset[0][:,:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile('rmsprop', 'mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = keras.preprocessing.sequence.pad_sequences([d[:, :-1] for d in dataset], dtype='float32', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:len(X)*3//4]\n",
    "Y_train = labels[:len(X)*3//4]\n",
    "X_test = X[len(X)*3//4:]\n",
    "Y_test = labels[len(X)*3//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cb = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=100)]\n",
    "# cb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/1000\n",
      "0s - loss: 0.3165 - acc: 0.4578 - val_loss: 0.2759 - val_acc: 0.4867\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.2785 - acc: 0.4622 - val_loss: 0.2555 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.2597 - acc: 0.4889 - val_loss: 0.2477 - val_acc: 0.5733\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.2507 - acc: 0.5711 - val_loss: 0.2474 - val_acc: 0.5600\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.2469 - acc: 0.5822 - val_loss: 0.2467 - val_acc: 0.5533\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.2442 - acc: 0.6022 - val_loss: 0.2455 - val_acc: 0.5800\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.2423 - acc: 0.6333 - val_loss: 0.2451 - val_acc: 0.5667\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.2403 - acc: 0.6267 - val_loss: 0.2445 - val_acc: 0.5667\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.2388 - acc: 0.6244 - val_loss: 0.2432 - val_acc: 0.5867\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.2379 - acc: 0.6533 - val_loss: 0.2432 - val_acc: 0.5800\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.2370 - acc: 0.6400 - val_loss: 0.2432 - val_acc: 0.5467\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.2359 - acc: 0.6267 - val_loss: 0.2436 - val_acc: 0.5600\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.2351 - acc: 0.6244 - val_loss: 0.2424 - val_acc: 0.5667\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.2343 - acc: 0.6489 - val_loss: 0.2418 - val_acc: 0.5733\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.2334 - acc: 0.6556 - val_loss: 0.2416 - val_acc: 0.5667\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.2325 - acc: 0.6289 - val_loss: 0.2408 - val_acc: 0.5667\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.2319 - acc: 0.6556 - val_loss: 0.2410 - val_acc: 0.5867\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.2313 - acc: 0.6644 - val_loss: 0.2405 - val_acc: 0.5800\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.2306 - acc: 0.6711 - val_loss: 0.2411 - val_acc: 0.5800\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.2300 - acc: 0.6489 - val_loss: 0.2407 - val_acc: 0.5600\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.2291 - acc: 0.6400 - val_loss: 0.2398 - val_acc: 0.5933\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.2285 - acc: 0.6556 - val_loss: 0.2391 - val_acc: 0.6067\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.2276 - acc: 0.6533 - val_loss: 0.2384 - val_acc: 0.6000\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.2269 - acc: 0.6822 - val_loss: 0.2378 - val_acc: 0.6000\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.2261 - acc: 0.6800 - val_loss: 0.2380 - val_acc: 0.5733\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.2253 - acc: 0.6889 - val_loss: 0.2384 - val_acc: 0.6000\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.2245 - acc: 0.6556 - val_loss: 0.2375 - val_acc: 0.6067\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.2239 - acc: 0.6778 - val_loss: 0.2367 - val_acc: 0.5800\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.2231 - acc: 0.6778 - val_loss: 0.2376 - val_acc: 0.5867\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.2222 - acc: 0.6578 - val_loss: 0.2372 - val_acc: 0.5867\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.2213 - acc: 0.6778 - val_loss: 0.2368 - val_acc: 0.5867\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.2209 - acc: 0.6600 - val_loss: 0.2347 - val_acc: 0.6000\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.2197 - acc: 0.6978 - val_loss: 0.2346 - val_acc: 0.6000\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.2190 - acc: 0.6844 - val_loss: 0.2334 - val_acc: 0.6000\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.2183 - acc: 0.7000 - val_loss: 0.2322 - val_acc: 0.6067\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.2177 - acc: 0.7022 - val_loss: 0.2331 - val_acc: 0.6067\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.2170 - acc: 0.6889 - val_loss: 0.2334 - val_acc: 0.6133\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.2162 - acc: 0.7000 - val_loss: 0.2329 - val_acc: 0.6133\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.2153 - acc: 0.6911 - val_loss: 0.2328 - val_acc: 0.6133\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.2144 - acc: 0.6933 - val_loss: 0.2312 - val_acc: 0.6200\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.2137 - acc: 0.7067 - val_loss: 0.2314 - val_acc: 0.6200\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.2128 - acc: 0.7111 - val_loss: 0.2321 - val_acc: 0.6000\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.2119 - acc: 0.6867 - val_loss: 0.2313 - val_acc: 0.6267\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.2114 - acc: 0.7044 - val_loss: 0.2298 - val_acc: 0.6200\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.2107 - acc: 0.7022 - val_loss: 0.2297 - val_acc: 0.6200\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.2098 - acc: 0.7067 - val_loss: 0.2293 - val_acc: 0.6267\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.2090 - acc: 0.7044 - val_loss: 0.2303 - val_acc: 0.6267\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.2082 - acc: 0.7200 - val_loss: 0.2294 - val_acc: 0.6267\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.2079 - acc: 0.7089 - val_loss: 0.2270 - val_acc: 0.6533\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.2070 - acc: 0.7400 - val_loss: 0.2279 - val_acc: 0.6467\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.2065 - acc: 0.7422 - val_loss: 0.2283 - val_acc: 0.6400\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.2057 - acc: 0.7178 - val_loss: 0.2263 - val_acc: 0.6333\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.2054 - acc: 0.7733 - val_loss: 0.2268 - val_acc: 0.6467\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.2048 - acc: 0.7489 - val_loss: 0.2267 - val_acc: 0.6533\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.2042 - acc: 0.7400 - val_loss: 0.2267 - val_acc: 0.6533\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.2036 - acc: 0.7400 - val_loss: 0.2263 - val_acc: 0.6533\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.2029 - acc: 0.7489 - val_loss: 0.2263 - val_acc: 0.6467\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.2024 - acc: 0.7200 - val_loss: 0.2256 - val_acc: 0.6533\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.2016 - acc: 0.7400 - val_loss: 0.2255 - val_acc: 0.6667\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.2012 - acc: 0.7689 - val_loss: 0.2257 - val_acc: 0.6400\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.2005 - acc: 0.7467 - val_loss: 0.2255 - val_acc: 0.6533\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.2000 - acc: 0.7489 - val_loss: 0.2250 - val_acc: 0.6467\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.1992 - acc: 0.7511 - val_loss: 0.2238 - val_acc: 0.6600\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.1987 - acc: 0.7800 - val_loss: 0.2251 - val_acc: 0.6267\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.1982 - acc: 0.7556 - val_loss: 0.2249 - val_acc: 0.6467\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.1974 - acc: 0.7689 - val_loss: 0.2242 - val_acc: 0.6533\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.1969 - acc: 0.7600 - val_loss: 0.2241 - val_acc: 0.6333\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.1962 - acc: 0.7622 - val_loss: 0.2228 - val_acc: 0.6200\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.1955 - acc: 0.7489 - val_loss: 0.2228 - val_acc: 0.6200\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.1950 - acc: 0.7533 - val_loss: 0.2233 - val_acc: 0.6200\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.1944 - acc: 0.7533 - val_loss: 0.2222 - val_acc: 0.6267\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.1938 - acc: 0.7822 - val_loss: 0.2229 - val_acc: 0.6200\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.1933 - acc: 0.7556 - val_loss: 0.2204 - val_acc: 0.6467\n",
      "Epoch 74/1000\n",
      "0s - loss: 0.1928 - acc: 0.7889 - val_loss: 0.2210 - val_acc: 0.6400\n",
      "Epoch 75/1000\n",
      "0s - loss: 0.1921 - acc: 0.7711 - val_loss: 0.2204 - val_acc: 0.6400\n",
      "Epoch 76/1000\n",
      "0s - loss: 0.1917 - acc: 0.8044 - val_loss: 0.2233 - val_acc: 0.6133\n",
      "Epoch 77/1000\n",
      "0s - loss: 0.1912 - acc: 0.7400 - val_loss: 0.2207 - val_acc: 0.6467\n",
      "Epoch 78/1000\n",
      "0s - loss: 0.1910 - acc: 0.8089 - val_loss: 0.2206 - val_acc: 0.6333\n",
      "Epoch 79/1000\n",
      "0s - loss: 0.1901 - acc: 0.8089 - val_loss: 0.2229 - val_acc: 0.6133\n",
      "Epoch 80/1000\n",
      "0s - loss: 0.1901 - acc: 0.7533 - val_loss: 0.2228 - val_acc: 0.6200\n",
      "Epoch 81/1000\n",
      "0s - loss: 0.1896 - acc: 0.7800 - val_loss: 0.2218 - val_acc: 0.6200\n",
      "Epoch 82/1000\n",
      "0s - loss: 0.1891 - acc: 0.7778 - val_loss: 0.2220 - val_acc: 0.6200\n",
      "Epoch 83/1000\n",
      "0s - loss: 0.1887 - acc: 0.7800 - val_loss: 0.2188 - val_acc: 0.6667\n",
      "Epoch 84/1000\n",
      "0s - loss: 0.1883 - acc: 0.8044 - val_loss: 0.2209 - val_acc: 0.6400\n",
      "Epoch 85/1000\n",
      "0s - loss: 0.1879 - acc: 0.7756 - val_loss: 0.2208 - val_acc: 0.6267\n",
      "Epoch 86/1000\n",
      "0s - loss: 0.1873 - acc: 0.7822 - val_loss: 0.2192 - val_acc: 0.6600\n",
      "Epoch 87/1000\n",
      "0s - loss: 0.1869 - acc: 0.8000 - val_loss: 0.2187 - val_acc: 0.6733\n",
      "Epoch 88/1000\n",
      "0s - loss: 0.1865 - acc: 0.8111 - val_loss: 0.2197 - val_acc: 0.6400\n",
      "Epoch 89/1000\n",
      "0s - loss: 0.1863 - acc: 0.7800 - val_loss: 0.2188 - val_acc: 0.6533\n",
      "Epoch 90/1000\n",
      "0s - loss: 0.1857 - acc: 0.7956 - val_loss: 0.2194 - val_acc: 0.6600\n",
      "Epoch 91/1000\n",
      "0s - loss: 0.1854 - acc: 0.8067 - val_loss: 0.2196 - val_acc: 0.6467\n",
      "Epoch 92/1000\n",
      "0s - loss: 0.1850 - acc: 0.8000 - val_loss: 0.2215 - val_acc: 0.6400\n",
      "Epoch 93/1000\n",
      "0s - loss: 0.1845 - acc: 0.7756 - val_loss: 0.2205 - val_acc: 0.6467\n",
      "Epoch 94/1000\n",
      "0s - loss: 0.1837 - acc: 0.7978 - val_loss: 0.2200 - val_acc: 0.6333\n",
      "Epoch 95/1000\n",
      "0s - loss: 0.1835 - acc: 0.8000 - val_loss: 0.2210 - val_acc: 0.6400\n",
      "Epoch 96/1000\n",
      "0s - loss: 0.1833 - acc: 0.7978 - val_loss: 0.2196 - val_acc: 0.6400\n",
      "Epoch 97/1000\n",
      "0s - loss: 0.1827 - acc: 0.8000 - val_loss: 0.2187 - val_acc: 0.6467\n",
      "Epoch 98/1000\n",
      "0s - loss: 0.1822 - acc: 0.7956 - val_loss: 0.2181 - val_acc: 0.6600\n",
      "Epoch 99/1000\n",
      "0s - loss: 0.1819 - acc: 0.8156 - val_loss: 0.2182 - val_acc: 0.6733\n",
      "Epoch 100/1000\n",
      "0s - loss: 0.1814 - acc: 0.8200 - val_loss: 0.2181 - val_acc: 0.6600\n",
      "Epoch 101/1000\n",
      "0s - loss: 0.1810 - acc: 0.8178 - val_loss: 0.2196 - val_acc: 0.6400\n",
      "Epoch 102/1000\n",
      "0s - loss: 0.1804 - acc: 0.8044 - val_loss: 0.2185 - val_acc: 0.6733\n",
      "Epoch 103/1000\n",
      "0s - loss: 0.1800 - acc: 0.8289 - val_loss: 0.2195 - val_acc: 0.6400\n",
      "Epoch 104/1000\n",
      "0s - loss: 0.1798 - acc: 0.8067 - val_loss: 0.2179 - val_acc: 0.6533\n",
      "Epoch 105/1000\n",
      "0s - loss: 0.1797 - acc: 0.8289 - val_loss: 0.2189 - val_acc: 0.6467\n",
      "Epoch 106/1000\n",
      "0s - loss: 0.1791 - acc: 0.8244 - val_loss: 0.2221 - val_acc: 0.6267\n",
      "Epoch 107/1000\n",
      "0s - loss: 0.1793 - acc: 0.8000 - val_loss: 0.2182 - val_acc: 0.6733\n",
      "Epoch 108/1000\n",
      "0s - loss: 0.1786 - acc: 0.8378 - val_loss: 0.2198 - val_acc: 0.6400\n",
      "Epoch 109/1000\n",
      "0s - loss: 0.1784 - acc: 0.8156 - val_loss: 0.2203 - val_acc: 0.6333\n",
      "Epoch 110/1000\n",
      "0s - loss: 0.1780 - acc: 0.8200 - val_loss: 0.2204 - val_acc: 0.6333\n",
      "Epoch 111/1000\n",
      "0s - loss: 0.1778 - acc: 0.8089 - val_loss: 0.2184 - val_acc: 0.6600\n",
      "Epoch 112/1000\n",
      "0s - loss: 0.1773 - acc: 0.8133 - val_loss: 0.2171 - val_acc: 0.6600\n",
      "Epoch 113/1000\n",
      "0s - loss: 0.1771 - acc: 0.8311 - val_loss: 0.2200 - val_acc: 0.6533\n",
      "Epoch 114/1000\n",
      "0s - loss: 0.1764 - acc: 0.8178 - val_loss: 0.2190 - val_acc: 0.6400\n",
      "Epoch 115/1000\n",
      "0s - loss: 0.1762 - acc: 0.8156 - val_loss: 0.2164 - val_acc: 0.6800\n",
      "Epoch 116/1000\n",
      "0s - loss: 0.1761 - acc: 0.8556 - val_loss: 0.2185 - val_acc: 0.6533\n",
      "Epoch 117/1000\n",
      "0s - loss: 0.1755 - acc: 0.8356 - val_loss: 0.2185 - val_acc: 0.6667\n",
      "Epoch 118/1000\n",
      "0s - loss: 0.1752 - acc: 0.8289 - val_loss: 0.2179 - val_acc: 0.6600\n",
      "Epoch 119/1000\n",
      "0s - loss: 0.1749 - acc: 0.8622 - val_loss: 0.2221 - val_acc: 0.6067\n",
      "Epoch 120/1000\n",
      "0s - loss: 0.1750 - acc: 0.7956 - val_loss: 0.2185 - val_acc: 0.6733\n",
      "Epoch 121/1000\n",
      "0s - loss: 0.1742 - acc: 0.8200 - val_loss: 0.2170 - val_acc: 0.6800\n",
      "Epoch 122/1000\n",
      "0s - loss: 0.1743 - acc: 0.8667 - val_loss: 0.2188 - val_acc: 0.6667\n",
      "Epoch 123/1000\n",
      "0s - loss: 0.1740 - acc: 0.8244 - val_loss: 0.2176 - val_acc: 0.6867\n",
      "Epoch 124/1000\n",
      "0s - loss: 0.1735 - acc: 0.8400 - val_loss: 0.2201 - val_acc: 0.6400\n",
      "Epoch 125/1000\n",
      "0s - loss: 0.1734 - acc: 0.8267 - val_loss: 0.2190 - val_acc: 0.6467\n",
      "Epoch 126/1000\n",
      "0s - loss: 0.1728 - acc: 0.8289 - val_loss: 0.2183 - val_acc: 0.6667\n",
      "Epoch 127/1000\n",
      "0s - loss: 0.1725 - acc: 0.8356 - val_loss: 0.2192 - val_acc: 0.6467\n",
      "Epoch 128/1000\n",
      "0s - loss: 0.1723 - acc: 0.8378 - val_loss: 0.2173 - val_acc: 0.6867\n",
      "Epoch 129/1000\n",
      "0s - loss: 0.1721 - acc: 0.8711 - val_loss: 0.2225 - val_acc: 0.6200\n",
      "Epoch 130/1000\n",
      "0s - loss: 0.1723 - acc: 0.8156 - val_loss: 0.2180 - val_acc: 0.6667\n",
      "Epoch 131/1000\n",
      "0s - loss: 0.1715 - acc: 0.8533 - val_loss: 0.2184 - val_acc: 0.6533\n",
      "Epoch 132/1000\n",
      "0s - loss: 0.1714 - acc: 0.8289 - val_loss: 0.2157 - val_acc: 0.6867\n",
      "Epoch 133/1000\n",
      "0s - loss: 0.1710 - acc: 0.8889 - val_loss: 0.2200 - val_acc: 0.6333\n",
      "Epoch 134/1000\n",
      "0s - loss: 0.1704 - acc: 0.8356 - val_loss: 0.2187 - val_acc: 0.6667\n",
      "Epoch 135/1000\n",
      "0s - loss: 0.1701 - acc: 0.8578 - val_loss: 0.2203 - val_acc: 0.6333\n",
      "Epoch 136/1000\n",
      "0s - loss: 0.1701 - acc: 0.8356 - val_loss: 0.2182 - val_acc: 0.6733\n",
      "Epoch 137/1000\n",
      "0s - loss: 0.1698 - acc: 0.8667 - val_loss: 0.2234 - val_acc: 0.6200\n",
      "Epoch 138/1000\n",
      "0s - loss: 0.1697 - acc: 0.8156 - val_loss: 0.2183 - val_acc: 0.6733\n",
      "Epoch 139/1000\n",
      "0s - loss: 0.1691 - acc: 0.8622 - val_loss: 0.2199 - val_acc: 0.6533\n",
      "Epoch 140/1000\n",
      "0s - loss: 0.1688 - acc: 0.8556 - val_loss: 0.2199 - val_acc: 0.6600\n",
      "Epoch 141/1000\n",
      "0s - loss: 0.1687 - acc: 0.8622 - val_loss: 0.2206 - val_acc: 0.6667\n",
      "Epoch 142/1000\n",
      "0s - loss: 0.1682 - acc: 0.8511 - val_loss: 0.2197 - val_acc: 0.6667\n",
      "Epoch 143/1000\n",
      "0s - loss: 0.1682 - acc: 0.8489 - val_loss: 0.2188 - val_acc: 0.6667\n",
      "Epoch 144/1000\n",
      "0s - loss: 0.1677 - acc: 0.8711 - val_loss: 0.2210 - val_acc: 0.6600\n",
      "Epoch 145/1000\n",
      "0s - loss: 0.1676 - acc: 0.8578 - val_loss: 0.2196 - val_acc: 0.6533\n",
      "Epoch 146/1000\n",
      "0s - loss: 0.1674 - acc: 0.8600 - val_loss: 0.2200 - val_acc: 0.6600\n",
      "Epoch 147/1000\n",
      "0s - loss: 0.1670 - acc: 0.8644 - val_loss: 0.2223 - val_acc: 0.6400\n",
      "Epoch 148/1000\n",
      "0s - loss: 0.1667 - acc: 0.8511 - val_loss: 0.2177 - val_acc: 0.6800\n",
      "Epoch 149/1000\n",
      "0s - loss: 0.1667 - acc: 0.8911 - val_loss: 0.2213 - val_acc: 0.6467\n",
      "Epoch 150/1000\n",
      "0s - loss: 0.1664 - acc: 0.8578 - val_loss: 0.2211 - val_acc: 0.6467\n",
      "Epoch 151/1000\n",
      "0s - loss: 0.1661 - acc: 0.8600 - val_loss: 0.2205 - val_acc: 0.6800\n",
      "Epoch 152/1000\n",
      "0s - loss: 0.1657 - acc: 0.8733 - val_loss: 0.2232 - val_acc: 0.6333\n",
      "Epoch 153/1000\n",
      "0s - loss: 0.1656 - acc: 0.8511 - val_loss: 0.2192 - val_acc: 0.6667\n",
      "Epoch 154/1000\n",
      "0s - loss: 0.1653 - acc: 0.8711 - val_loss: 0.2205 - val_acc: 0.6600\n",
      "Epoch 155/1000\n",
      "0s - loss: 0.1651 - acc: 0.8622 - val_loss: 0.2211 - val_acc: 0.6600\n",
      "Epoch 156/1000\n",
      "0s - loss: 0.1651 - acc: 0.8711 - val_loss: 0.2221 - val_acc: 0.6533\n",
      "Epoch 157/1000\n",
      "0s - loss: 0.1645 - acc: 0.8711 - val_loss: 0.2217 - val_acc: 0.6467\n",
      "Epoch 158/1000\n",
      "0s - loss: 0.1644 - acc: 0.8622 - val_loss: 0.2202 - val_acc: 0.6733\n",
      "Epoch 159/1000\n",
      "0s - loss: 0.1642 - acc: 0.8822 - val_loss: 0.2199 - val_acc: 0.6667\n",
      "Epoch 160/1000\n",
      "0s - loss: 0.1640 - acc: 0.8800 - val_loss: 0.2204 - val_acc: 0.6600\n",
      "Epoch 161/1000\n",
      "0s - loss: 0.1637 - acc: 0.8822 - val_loss: 0.2219 - val_acc: 0.6600\n",
      "Epoch 162/1000\n",
      "0s - loss: 0.1636 - acc: 0.8689 - val_loss: 0.2207 - val_acc: 0.6733\n",
      "Epoch 163/1000\n",
      "0s - loss: 0.1632 - acc: 0.8844 - val_loss: 0.2228 - val_acc: 0.6467\n",
      "Epoch 164/1000\n",
      "0s - loss: 0.1633 - acc: 0.8756 - val_loss: 0.2199 - val_acc: 0.6733\n",
      "Epoch 165/1000\n",
      "0s - loss: 0.1629 - acc: 0.8933 - val_loss: 0.2232 - val_acc: 0.6467\n",
      "Epoch 166/1000\n",
      "0s - loss: 0.1626 - acc: 0.8622 - val_loss: 0.2205 - val_acc: 0.6800\n",
      "Epoch 167/1000\n",
      "0s - loss: 0.1627 - acc: 0.8822 - val_loss: 0.2228 - val_acc: 0.6600\n",
      "Epoch 168/1000\n",
      "0s - loss: 0.1622 - acc: 0.8867 - val_loss: 0.2229 - val_acc: 0.6467\n",
      "Epoch 169/1000\n",
      "0s - loss: 0.1620 - acc: 0.8733 - val_loss: 0.2227 - val_acc: 0.6533\n",
      "Epoch 170/1000\n",
      "0s - loss: 0.1618 - acc: 0.8756 - val_loss: 0.2223 - val_acc: 0.6600\n",
      "Epoch 171/1000\n",
      "0s - loss: 0.1617 - acc: 0.8956 - val_loss: 0.2234 - val_acc: 0.6467\n",
      "Epoch 172/1000\n",
      "0s - loss: 0.1614 - acc: 0.8756 - val_loss: 0.2224 - val_acc: 0.6733\n",
      "Epoch 173/1000\n",
      "0s - loss: 0.1613 - acc: 0.8867 - val_loss: 0.2245 - val_acc: 0.6467\n",
      "Epoch 174/1000\n",
      "0s - loss: 0.1610 - acc: 0.8756 - val_loss: 0.2235 - val_acc: 0.6533\n",
      "Epoch 175/1000\n",
      "0s - loss: 0.1608 - acc: 0.8822 - val_loss: 0.2203 - val_acc: 0.6667\n",
      "Epoch 176/1000\n",
      "0s - loss: 0.1606 - acc: 0.9089 - val_loss: 0.2269 - val_acc: 0.6467\n",
      "Epoch 177/1000\n",
      "0s - loss: 0.1604 - acc: 0.8778 - val_loss: 0.2226 - val_acc: 0.6800\n",
      "Epoch 178/1000\n",
      "0s - loss: 0.1604 - acc: 0.8956 - val_loss: 0.2228 - val_acc: 0.6800\n",
      "Epoch 179/1000\n",
      "0s - loss: 0.1599 - acc: 0.8889 - val_loss: 0.2234 - val_acc: 0.6533\n",
      "Epoch 180/1000\n",
      "0s - loss: 0.1598 - acc: 0.8933 - val_loss: 0.2256 - val_acc: 0.6600\n",
      "Epoch 181/1000\n",
      "0s - loss: 0.1599 - acc: 0.8822 - val_loss: 0.2223 - val_acc: 0.6600\n",
      "Epoch 182/1000\n",
      "0s - loss: 0.1596 - acc: 0.8867 - val_loss: 0.2257 - val_acc: 0.6533\n",
      "Epoch 183/1000\n",
      "0s - loss: 0.1594 - acc: 0.8844 - val_loss: 0.2237 - val_acc: 0.6600\n",
      "Epoch 184/1000\n",
      "0s - loss: 0.1592 - acc: 0.9000 - val_loss: 0.2248 - val_acc: 0.6600\n",
      "Epoch 185/1000\n",
      "0s - loss: 0.1590 - acc: 0.8911 - val_loss: 0.2256 - val_acc: 0.6667\n",
      "Epoch 186/1000\n",
      "0s - loss: 0.1589 - acc: 0.8889 - val_loss: 0.2242 - val_acc: 0.6600\n",
      "Epoch 187/1000\n",
      "0s - loss: 0.1584 - acc: 0.9044 - val_loss: 0.2272 - val_acc: 0.6467\n",
      "Epoch 188/1000\n",
      "0s - loss: 0.1584 - acc: 0.8844 - val_loss: 0.2248 - val_acc: 0.6533\n",
      "Epoch 189/1000\n",
      "0s - loss: 0.1583 - acc: 0.8867 - val_loss: 0.2235 - val_acc: 0.6667\n",
      "Epoch 190/1000\n",
      "0s - loss: 0.1583 - acc: 0.8978 - val_loss: 0.2235 - val_acc: 0.6733\n",
      "Epoch 191/1000\n",
      "0s - loss: 0.1580 - acc: 0.9222 - val_loss: 0.2276 - val_acc: 0.6467\n",
      "Epoch 192/1000\n",
      "0s - loss: 0.1580 - acc: 0.8978 - val_loss: 0.2262 - val_acc: 0.6533\n",
      "Epoch 193/1000\n",
      "0s - loss: 0.1577 - acc: 0.9000 - val_loss: 0.2251 - val_acc: 0.6600\n",
      "Epoch 194/1000\n",
      "0s - loss: 0.1576 - acc: 0.9089 - val_loss: 0.2258 - val_acc: 0.6533\n",
      "Epoch 195/1000\n",
      "0s - loss: 0.1578 - acc: 0.9111 - val_loss: 0.2272 - val_acc: 0.6533\n",
      "Epoch 196/1000\n",
      "0s - loss: 0.1573 - acc: 0.8978 - val_loss: 0.2250 - val_acc: 0.6533\n",
      "Epoch 197/1000\n",
      "0s - loss: 0.1573 - acc: 0.8956 - val_loss: 0.2231 - val_acc: 0.6733\n",
      "Epoch 198/1000\n",
      "0s - loss: 0.1574 - acc: 0.9200 - val_loss: 0.2266 - val_acc: 0.6467\n",
      "Epoch 199/1000\n",
      "0s - loss: 0.1571 - acc: 0.8889 - val_loss: 0.2233 - val_acc: 0.6533\n",
      "Epoch 200/1000\n",
      "0s - loss: 0.1571 - acc: 0.9289 - val_loss: 0.2305 - val_acc: 0.6333\n",
      "Epoch 201/1000\n",
      "0s - loss: 0.1571 - acc: 0.8867 - val_loss: 0.2266 - val_acc: 0.6400\n",
      "Epoch 202/1000\n",
      "0s - loss: 0.1565 - acc: 0.9067 - val_loss: 0.2317 - val_acc: 0.6333\n",
      "Epoch 203/1000\n",
      "0s - loss: 0.1568 - acc: 0.8889 - val_loss: 0.2289 - val_acc: 0.6467\n",
      "Epoch 204/1000\n",
      "0s - loss: 0.1564 - acc: 0.8978 - val_loss: 0.2286 - val_acc: 0.6467\n",
      "Epoch 205/1000\n",
      "0s - loss: 0.1561 - acc: 0.8956 - val_loss: 0.2257 - val_acc: 0.6533\n",
      "Epoch 206/1000\n",
      "0s - loss: 0.1561 - acc: 0.9200 - val_loss: 0.2291 - val_acc: 0.6467\n",
      "Epoch 207/1000\n",
      "0s - loss: 0.1563 - acc: 0.8933 - val_loss: 0.2265 - val_acc: 0.6467\n",
      "Epoch 208/1000\n",
      "0s - loss: 0.1558 - acc: 0.9089 - val_loss: 0.2288 - val_acc: 0.6333\n",
      "Epoch 209/1000\n",
      "0s - loss: 0.1560 - acc: 0.8978 - val_loss: 0.2268 - val_acc: 0.6400\n",
      "Epoch 210/1000\n",
      "0s - loss: 0.1555 - acc: 0.9133 - val_loss: 0.2281 - val_acc: 0.6533\n",
      "Epoch 211/1000\n",
      "0s - loss: 0.1555 - acc: 0.9067 - val_loss: 0.2289 - val_acc: 0.6600\n",
      "Epoch 212/1000\n",
      "0s - loss: 0.1554 - acc: 0.9022 - val_loss: 0.2294 - val_acc: 0.6600\n",
      "Epoch 213/1000\n",
      "0s - loss: 0.1551 - acc: 0.9000 - val_loss: 0.2271 - val_acc: 0.6667\n",
      "Epoch 214/1000\n",
      "0s - loss: 0.1550 - acc: 0.9133 - val_loss: 0.2313 - val_acc: 0.6600\n",
      "Epoch 215/1000\n",
      "0s - loss: 0.1547 - acc: 0.9111 - val_loss: 0.2299 - val_acc: 0.6600\n",
      "Epoch 216/1000\n",
      "0s - loss: 0.1547 - acc: 0.9089 - val_loss: 0.2282 - val_acc: 0.6533\n",
      "Epoch 217/1000\n",
      "0s - loss: 0.1548 - acc: 0.9200 - val_loss: 0.2306 - val_acc: 0.6533\n",
      "Epoch 218/1000\n",
      "0s - loss: 0.1545 - acc: 0.9089 - val_loss: 0.2275 - val_acc: 0.6600\n",
      "Epoch 219/1000\n",
      "0s - loss: 0.1544 - acc: 0.9200 - val_loss: 0.2312 - val_acc: 0.6533\n",
      "Epoch 220/1000\n",
      "0s - loss: 0.1544 - acc: 0.9133 - val_loss: 0.2316 - val_acc: 0.6600\n",
      "Epoch 221/1000\n",
      "0s - loss: 0.1544 - acc: 0.9111 - val_loss: 0.2304 - val_acc: 0.6600\n",
      "Epoch 222/1000\n",
      "0s - loss: 0.1540 - acc: 0.9156 - val_loss: 0.2308 - val_acc: 0.6533\n",
      "Epoch 223/1000\n",
      "0s - loss: 0.1541 - acc: 0.9089 - val_loss: 0.2295 - val_acc: 0.6733\n",
      "Epoch 224/1000\n",
      "0s - loss: 0.1541 - acc: 0.9244 - val_loss: 0.2310 - val_acc: 0.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20664b38fd0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=32, epochs=1000, validation_data=(X_test, Y_test), callbacks=cb, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
