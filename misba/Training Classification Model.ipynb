{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from scipy.io import loadmat, savemat\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Running the classification form of the model free of classes\n",
    "In this notebook, we construct the model purely as a classifier with no pretraining. We have ample data to do this synthetically, so we can show the validity of the model. \n",
    "\n",
    "Again, we're assuming something about the data. Namely, there are \n",
    " * $K$ kinds of instances, each with $N_k$ instances.\n",
    " * Each kind will have $M_k$ features\n",
    " * There is an assumed latent binary label for each instance, $Y_{k, n}$.\n",
    " * The label of the total bag will be $0$ if and only if all $Y_{k, n} = 0$\n",
    " \n",
    "Note that the model complexity is very small, but capable of managing the multiple-instance structure of the data which gives the data so many raw features. If we have hidden layers of size $h$, there are only $$h \\sum_0^{K-1}{(M_k+N_k)}$$ parameters. This is due to the convolutional first layer, which replaces a dense first layer having $$h \\sum_0^{K-1}{(M_k \\cdot N_k)}$$ parameters. The reduction in parameters is intentional in order to reduce overfitting especially when there are very few training examples.\n",
    "\n",
    "The second formulation of the model even further reduces the number of parameters while retaining the ability to correcly classify all of the examples by weighting all baskets the same. We assume $h=1$ and introduce a learned scaling parameter that scales the sum of the submodels to make a consensus. Thus, we have $$\\sum_0^{K-1}{M_k}$$ parameters. This reduction in parameters not only protects against overfitting, but also reduces the complexity of training - the fewer parameters, the easier it is to train, generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "K = 2\n",
    "M = [30, 35]\n",
    "N = [10, 15]\n",
    "batch_size = 10\n",
    "hidden_layer_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return 1 - ((np.round(y_pred) - y_true)**2).sum() / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "synth_data = loadmat('./synth_dataset.mat')\n",
    "synth_data\n",
    "\n",
    "trainX = synth_data['train_X']\n",
    "trainY = np.ravel(synth_data['train_Y'])\n",
    "\n",
    "testX = synth_data['test_X'].reshape(-1, sum(m*n for m, n in zip(M, N)), 1)\n",
    "testY = np.ravel(synth_data['test_Y']).reshape(-1, 1)\n",
    "\n",
    "\n",
    "shuffle_idx = np.random.permutation(np.arange(len(trainX)))\n",
    "trainX = trainX[shuffle_idx]\n",
    "trainY = trainY[shuffle_idx]\n",
    "validX = trainX[:len(trainX)//4].reshape(-1, sum(m*n for m, n in zip(M, N)), 1)\n",
    "validY = trainY[:len(trainY)//4].reshape(-1, 1)\n",
    "trainX = trainX[len(trainX)//4:].reshape(-1, sum(m*n for m, n in zip(M, N)), 1)\n",
    "trainY = trainY[len(trainY)//4:].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False, False, False,  True, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "        True,  True, False,  True, False,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True, False, False, False,\n",
       "        True, False,  True,  True,  True, False, False,  True, False,\n",
       "       False, False,  True,  True, False,  True,  True,  True,  True,\n",
       "       False, False,  True, False,  True,  True,  True, False, False,\n",
       "       False, False, False,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True, False, False, False, False,  True, False,\n",
       "       False,  True, False, False, False,  True,  True, False, False,\n",
       "        True, False, False, False,  True,  True, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False, False,  True, False, False,  True, False, False, False,\n",
       "        True,  True, False,  True,  True, False,  True,  True, False,\n",
       "        True,  True, False,  True, False,  True, False, False, False,\n",
       "        True,  True, False, False,  True, False,  True, False,  True,\n",
       "        True, False,  True,  True, False,  True, False, False,  True,\n",
       "       False, False, False,  True, False,  True,  True, False, False,\n",
       "        True,  True,  True, False, False,  True,  True, False,  True,\n",
       "        True, False,  True,  True,  True, False,  True, False, False,\n",
       "        True,  True,  True, False,  True, False, False,  True, False,\n",
       "        True,  True, False,  True,  True, False, False,  True, False,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False, False,  True, False,  True,  True,  True,  True, False,\n",
       "        True,  True, False, False,  True, False, False, False, False,\n",
       "        True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True,  True,  True, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ravel(testYFull.max(axis=1)) == np.ravel(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 825, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    idx = sum(N[_k]*M[_k] for _k in range(0, k))\n",
    "    next_idx = N[k]*M[k] + idx\n",
    "    trainX[:, idx:next_idx].reshape((len(trainX), N[k], M[k])).mean(axis=(0, 1))\n",
    "    q1 = np.percentile(trainX[:, idx:next_idx].reshape((len(trainX), N[k], M[k])), 25, axis=(0, 1))\n",
    "    q3 = np.percentile(trainX[:, idx:next_idx].reshape((len(trainX), N[k], M[k])), 75, axis=(0, 1))\n",
    "    q2 = np.percentile(trainX[:, idx:next_idx].reshape((len(trainX), N[k], M[k])), 50, axis=(0, 1))\n",
    "    trainX[:, idx:next_idx] = ((trainX[:, idx:next_idx].reshape((len(trainX), N[k], M[k])) - q3) / (q3 - q1)).reshape((len(trainX), -1, 1))\n",
    "    validX[:, idx:next_idx] = ((validX[:, idx:next_idx].reshape((len(validX), N[k], M[k])) - q3) / (q3 - q1)).reshape((len(validX), -1, 1))\n",
    "    testX[:, idx:next_idx] = ((testX[:, idx:next_idx].reshape((len(testX), N[k], M[k])) - q3) / (q3 - q1)).reshape((len(testX), -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, sum(m*n for m,n in zip(M, N)), 1))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    tf_valid_dataset = tf.constant(validX, tf.float32)\n",
    "    tf_valid_labels = tf.constant(validY.reshape(-1,1), tf.float32)\n",
    "    tf_test_dataset = tf.constant(testX, tf.float32)\n",
    "    \n",
    "    layer0_weights = []\n",
    "    layer0_biases = []\n",
    "    for k in range(K):\n",
    "        kind_weights = tf.Variable(tf.truncated_normal([M[k], 1, hidden_layer_size], stddev=0.1))\n",
    "        layer0_weights.append(kind_weights)\n",
    "        \n",
    "        kind_biases = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        layer0_biases.append(kind_biases)\n",
    "        \n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([sum(N[k]*hidden_layer_size for k in range(K)), 1], stddev=0.1)) \n",
    "    layer1_biases = tf.Variable(tf.zeros([1]))\n",
    "    \n",
    "    def model(data):\n",
    "        hidden_list = []\n",
    "        for k in range(K):\n",
    "            idx = sum(N[_k]*M[_k] for _k in range(0, k))\n",
    "            next_idx = N[k]*M[k] + idx\n",
    "            kind_conv = tf.nn.conv1d(data[:, idx:next_idx], layer0_weights[k], M[k], padding='VALID')\n",
    "            kind_hidden = tf.nn.relu(kind_conv + layer0_biases[k])\n",
    "            hidden_list.append(kind_hidden)\n",
    "\n",
    "        hidden = tf.reshape(tf.concat(hidden_list, 1), (-1, sum(N[k]*hidden_layer_size for k in range(K))))\n",
    "        logits = tf.matmul( hidden, layer1_weights) + layer1_biases\n",
    "        return logits\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))    \n",
    "    optimizer = tf.train.MomentumOptimizer(0.1, 0.7).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.sigmoid(logits)\n",
    "    valid_prediction = tf.nn.sigmoid(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.sigmoid(model(tf_test_dataset))\n",
    "    \n",
    "    train_accuracy = 1 - tf.reduce_mean(tf.square((tf.round(train_prediction) - tf_train_labels)))\n",
    "    valid_accuracy = 1 - tf.reduce_mean(tf.square((tf.round(valid_prediction) - tf_valid_labels)))\n",
    "#     test_accuracy = tf.reduce_sum(tf.square((tf.round(test_prediction) - tf)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 0.695005\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 49.2%\n",
      "Minibatch loss at step 200: 0.615917\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 400: 0.100412\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 600: 0.085719\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.3%\n",
      "Minibatch loss at step 800: 0.116186\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 1000: 0.024227\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1200: 0.030283\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1400: 0.008113\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1600: 0.013108\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1800: 0.016367\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2000: 0.005482\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2200: 0.007370\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2400: 0.007745\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2600: 0.006658\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2800: 0.007640\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3000: 0.004064\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3200: 0.004422\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3400: 0.002090\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3600: 0.002965\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3800: 0.002890\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4000: 0.002109\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4200: 0.003488\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4400: 0.004782\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4600: 0.001634\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4800: 0.002003\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5000: 0.002652\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5200: 0.002270\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5400: 0.004538\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5600: 0.001751\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5800: 0.005202\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6000: 0.000871\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6200: 0.001125\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6400: 0.003874\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6600: 0.001292\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6800: 0.000817\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7000: 0.000849\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7200: 0.001501\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7400: 0.000935\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7600: 0.002051\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7800: 0.001624\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000761\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8200: 0.001436\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8400: 0.001256\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8600: 0.001945\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8800: 0.000597\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9000: 0.000556\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9200: 0.001669\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9400: 0.001986\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9600: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9800: 0.001058\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 10000: 0.000743\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (trainY.shape[0] - batch_size)\n",
    "        batch_data = trainX[offset:(offset+batch_size)]\n",
    "        batch_labels = trainY[offset:(offset+batch_size)]\n",
    "        feed_dict = {tf_train_dataset: batch_data, \n",
    "                     tf_train_labels: batch_labels}\n",
    "        _, l, batch_pred = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % (accuracy(batch_labels, batch_pred)*100))\n",
    "            print('Validation accuracy: %.1f%%' % (valid_accuracy.eval()*100))\n",
    "    print('Test accuracy: %.1f%%' % (accuracy(testY,test_prediction.eval())*100))       \n",
    "    print('Test ROC AUC: %.1f%%' % (roc_auc_score(testY, test_prediction.eval())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# This time there are no weights deciding which submodel to use. It's just a consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, sum(m*n for m,n in zip(M, N)), 1))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    tf_valid_dataset = tf.constant(validX, tf.float32)\n",
    "    tf_valid_labels = tf.constant(validY.reshape(-1,1), tf.float32)\n",
    "    tf_test_dataset = tf.constant(testX, tf.float32)\n",
    "    \n",
    "    layer0_weights = []\n",
    "    layer0_biases = []\n",
    "    for k in range(K):\n",
    "        kind_weights = tf.Variable(tf.truncated_normal([M[k], 1, hidden_layer_size], stddev=0.1))\n",
    "        layer0_weights.append(kind_weights)\n",
    "        \n",
    "        kind_biases = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        layer0_biases.append(kind_biases)\n",
    "        \n",
    "    layer1_weights = tf.constant(np.ones((sum(N[_k] for _k in range(0, K)), 1)), dtype=tf.float32)\n",
    "    normalizer = tf.Variable(1.0, dtype=tf.float32)\n",
    "    layer1_biases = tf.Variable(tf.zeros([1]))\n",
    "    \n",
    "    def model(data):\n",
    "        hidden_list = []\n",
    "        for k in range(K):\n",
    "            idx = sum(N[_k]*M[_k] for _k in range(0, k))\n",
    "            next_idx = N[k]*M[k] + idx\n",
    "            kind_conv = tf.nn.conv1d(data[:, idx:next_idx], layer0_weights[k], M[k], padding='VALID')\n",
    "            kind_hidden = tf.nn.sigmoid(kind_conv + layer0_biases[k])\n",
    "            hidden_list.append(kind_hidden)\n",
    "    \n",
    "        hidden = tf.reshape(tf.concat(hidden_list, 1), (-1, sum(N[k]*hidden_layer_size for k in range(K))))\n",
    "        logits = tf.matmul( hidden, layer1_weights)*normalizer + layer1_biases\n",
    "        return logits\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "#            tf.nn.l2_loss(layer0_weights[0])*0.1 + tf.nn.l2_loss(layer0_weights[1])*0.1\n",
    "    optimizer = tf.train.MomentumOptimizer(0.1, 0.7).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.sigmoid(logits)\n",
    "    valid_prediction = tf.nn.sigmoid(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.sigmoid(model(tf_test_dataset))\n",
    "    \n",
    "    train_accuracy = 1 - tf.reduce_mean(tf.square((tf.round(train_prediction) - tf_train_labels)))\n",
    "    valid_accuracy = 1 - tf.reduce_mean(tf.square((tf.round(valid_prediction) - tf_valid_labels)))\n",
    "#     test_accuracy = tf.reduce_sum(tf.square((tf.round(test_prediction) - tf)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 7.135017\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 55.1%\n",
      "Minibatch loss at step 200: 0.931169\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 400: 0.639737\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 600: 0.165660\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 96.3%\n",
      "Minibatch loss at step 800: 0.193759\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1000: 0.053783\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1200: 0.035740\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1400: 0.017797\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1600: 0.020802\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1800: 0.020199\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2000: 0.009804\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2200: 0.013518\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2400: 0.011211\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2600: 0.012323\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2800: 0.011646\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3000: 0.005573\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3200: 0.007332\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3400: 0.004488\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3600: 0.006952\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3800: 0.005061\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4000: 0.003865\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4200: 0.003563\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4400: 0.006917\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4600: 0.002343\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 4800: 0.003569\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5000: 0.005388\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5200: 0.004932\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5400: 0.006302\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5600: 0.004044\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 5800: 0.006326\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6000: 0.002087\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6200: 0.001745\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6400: 0.004884\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6600: 0.002236\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 6800: 0.001840\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7000: 0.002246\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7200: 0.002411\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7400: 0.001935\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7600: 0.002567\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 7800: 0.003341\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.002108\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8200: 0.002725\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8400: 0.002668\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8600: 0.003681\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 8800: 0.001644\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9000: 0.000810\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9200: 0.002322\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9400: 0.002018\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9600: 0.001225\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 9800: 0.001871\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 10000: 0.001795\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (trainY.shape[0] - batch_size)\n",
    "        batch_data = trainX[offset:(offset+batch_size)]\n",
    "        batch_labels = trainY[offset:(offset+batch_size)]\n",
    "        feed_dict = {tf_train_dataset: batch_data, \n",
    "                     tf_train_labels: batch_labels}\n",
    "        _, l, batch_pred = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "#         print('normalizer ', normalizer.eval())\n",
    "        \n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % (accuracy(batch_labels, batch_pred)*100))\n",
    "            print('Validation accuracy: %.1f%%' % (valid_accuracy.eval()*100))\n",
    "#             print('bias ', layer1_biases.eval())\n",
    "#             print(np.ravel(layer0_weights[0].eval()))\n",
    "#             print(np.ravel(layer0_weights[1].eval()))\n",
    "\n",
    "    print('Test accuracy: %.1f%%' % (accuracy(testY,test_prediction.eval())*100))       \n",
    "    print('Test ROC AUC: %.1f%%' % (roc_auc_score(testY, test_prediction.eval())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for sensitivity to amount of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "525\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "500\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "475\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "450\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "425\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "400\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "375\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "350\n",
      "Test accuracy: 99.2%\n",
      "Test ROC AUC: 100.0%\n",
      "325\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "300\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "275\n",
      "Test accuracy: 98.8%\n",
      "Test ROC AUC: 100.0%\n",
      "250\n",
      "Test accuracy: 99.2%\n",
      "Test ROC AUC: 100.0%\n",
      "225\n",
      "Test accuracy: 98.4%\n",
      "Test ROC AUC: 99.9%\n",
      "200\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "175\n",
      "Test accuracy: 98.8%\n",
      "Test ROC AUC: 100.0%\n",
      "150\n",
      "Test accuracy: 99.2%\n",
      "Test ROC AUC: 100.0%\n",
      "125\n",
      "Test accuracy: 98.8%\n",
      "Test ROC AUC: 99.9%\n",
      "100\n",
      "Test accuracy: 98.0%\n",
      "Test ROC AUC: 100.0%\n",
      "75\n",
      "Test accuracy: 95.6%\n",
      "Test ROC AUC: 98.5%\n",
      "50\n",
      "Test accuracy: 80.4%\n",
      "Test ROC AUC: 88.1%\n",
      "25\n",
      "Test accuracy: 69.2%\n",
      "Test ROC AUC: 77.4%\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "test_accuracies = []\n",
    "for size in [563]+list(range(525, 0, -25)):\n",
    "    print(size)\n",
    "    idx = np.random.choice(np.arange(563), replace=False, size=[size])\n",
    "    trainXSmaller = trainX[idx]\n",
    "    trainYSmaller = trainY[idx]\n",
    "    num_steps = 10001\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (trainYSmaller.shape[0] - batch_size)\n",
    "            batch_data = trainXSmaller[offset:(offset+batch_size)]\n",
    "            batch_labels = trainYSmaller[offset:(offset+batch_size)]\n",
    "            feed_dict = {tf_train_dataset: batch_data, \n",
    "                         tf_train_labels: batch_labels}\n",
    "            _, l, batch_pred = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #         print('normalizer ', normalizer.eval())\n",
    "\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                pass\n",
    "#                 print('Minibatch loss at step %d: %f' % (step, l))\n",
    "#                 print('Minibatch accuracy: %.1f%%' % (accuracy(batch_labels, batch_pred)*100))\n",
    "#                 print('Validation accuracy: %.1f%%' % (valid_accuracy.eval()*100))\n",
    "    #             print('bias ', layer1_biases.eval())\n",
    "    #             print(np.ravel(layer0_weights[0].eval()))\n",
    "    #             print(np.ravel(layer0_weights[1].eval()))\n",
    "\n",
    "        print('Test accuracy: %.1f%%' % (accuracy(testY,test_prediction.eval())*100))       \n",
    "        print('Test ROC AUC: %.1f%%' % (roc_auc_score(testY, test_prediction.eval())*100))\n",
    "        sizes.append(size)\n",
    "        test_accuracies.append(accuracy(testY,test_prediction.eval())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6P8D8KoWInpJw2gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce2c2d66d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(sizes, test_accuracies)\n",
    "plt.title('Accuracy VS Training set size')\n",
    "plt.ylabel('Test Set Accuracy (%)')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(30), Dimension(1), Dimension(1)])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0_weights[0].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(35), Dimension(1), Dimension(1)])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0_weights[1].get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefining the first model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, sum(m*n for m,n in zip(M, N)), 1))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    tf_valid_dataset = tf.constant(validX, tf.float32)\n",
    "    tf_valid_labels = tf.constant(validY.reshape(-1,1), tf.float32)\n",
    "    tf_test_dataset = tf.constant(testX, tf.float32)\n",
    "    \n",
    "    layer0_weights = []\n",
    "    layer0_biases = []\n",
    "    for k in range(K):\n",
    "        kind_weights = tf.Variable(tf.truncated_normal([M[k], 1, hidden_layer_size], stddev=0.1))\n",
    "        layer0_weights.append(kind_weights)\n",
    "        \n",
    "        kind_biases = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        layer0_biases.append(kind_biases)\n",
    "        \n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([sum(N[k]*hidden_layer_size for k in range(K)), 1], stddev=0.1)) \n",
    "    layer1_biases = tf.Variable(tf.zeros([1]))\n",
    "    \n",
    "    def model(data):\n",
    "        hidden_list = []\n",
    "        for k in range(K):\n",
    "            idx = sum(N[_k]*M[_k] for _k in range(0, k))\n",
    "            next_idx = N[k]*M[k] + idx\n",
    "            kind_conv = tf.nn.conv1d(data[:, idx:next_idx], layer0_weights[k], M[k], padding='VALID')\n",
    "            kind_hidden = tf.nn.relu(kind_conv + layer0_biases[k])\n",
    "            hidden_list.append(kind_hidden)\n",
    "\n",
    "        hidden = tf.reshape(tf.concat(hidden_list, 1), (-1, sum(N[k]*hidden_layer_size for k in range(K))))\n",
    "        logits = tf.matmul( hidden, layer1_weights) + layer1_biases\n",
    "        return logits\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))    \n",
    "    optimizer = tf.train.MomentumOptimizer(0.1, 0.7).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.sigmoid(logits)\n",
    "    valid_prediction = tf.nn.sigmoid(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.sigmoid(model(tf_test_dataset))\n",
    "    \n",
    "    train_accuracy = 1 - tf.reduce_mean(tf.square((tf.round(train_prediction) - tf_train_labels)))\n",
    "    valid_accuracy = 1 - tf.reduce_mean(tf.square((tf.round(valid_prediction) - tf_valid_labels)))\n",
    "#     test_accuracy = tf.reduce_sum(tf.square((tf.round(test_prediction) - tf)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "525\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "500\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "475\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "450\n",
      "Test accuracy: 99.2%\n",
      "Test ROC AUC: 100.0%\n",
      "425\n",
      "Test accuracy: 99.2%\n",
      "Test ROC AUC: 100.0%\n",
      "400\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "375\n",
      "Test accuracy: 99.6%\n",
      "Test ROC AUC: 100.0%\n",
      "350\n",
      "Test accuracy: 98.0%\n",
      "Test ROC AUC: 100.0%\n",
      "325\n",
      "Test accuracy: 99.2%\n",
      "Test ROC AUC: 100.0%\n",
      "300\n",
      "Test accuracy: 98.0%\n",
      "Test ROC AUC: 99.9%\n",
      "275\n",
      "Test accuracy: 96.8%\n",
      "Test ROC AUC: 99.5%\n",
      "250\n",
      "Test accuracy: 98.4%\n",
      "Test ROC AUC: 100.0%\n",
      "225\n",
      "Test accuracy: 96.4%\n",
      "Test ROC AUC: 99.8%\n",
      "200\n",
      "Test accuracy: 96.0%\n",
      "Test ROC AUC: 99.7%\n",
      "175\n",
      "Test accuracy: 94.8%\n",
      "Test ROC AUC: 98.9%\n",
      "150\n",
      "Test accuracy: 88.4%\n",
      "Test ROC AUC: 92.4%\n",
      "125\n",
      "Test accuracy: 52.8%\n",
      "Test ROC AUC: 54.1%\n",
      "100\n",
      "Test accuracy: 59.6%\n",
      "Test ROC AUC: 59.1%\n",
      "75\n",
      "Test accuracy: 51.6%\n",
      "Test ROC AUC: 53.5%\n",
      "50\n",
      "Test accuracy: 50.8%\n",
      "Test ROC AUC: 48.7%\n",
      "25\n",
      "Test accuracy: 52.8%\n",
      "Test ROC AUC: 51.1%\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "test_accuracies = []\n",
    "for size in [563]+list(range(525, 0, -25)):\n",
    "    print(size)\n",
    "    idx = np.random.choice(np.arange(563), replace=False, size=[size])\n",
    "    trainXSmaller = trainX[idx]\n",
    "    trainYSmaller = trainY[idx]\n",
    "    num_steps = 10001\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (trainYSmaller.shape[0] - batch_size)\n",
    "            batch_data = trainXSmaller[offset:(offset+batch_size)]\n",
    "            batch_labels = trainYSmaller[offset:(offset+batch_size)]\n",
    "            feed_dict = {tf_train_dataset: batch_data, \n",
    "                         tf_train_labels: batch_labels}\n",
    "            _, l, batch_pred = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #         print('normalizer ', normalizer.eval())\n",
    "\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                pass\n",
    "#                 print('Minibatch loss at step %d: %f' % (step, l))\n",
    "#                 print('Minibatch accuracy: %.1f%%' % (accuracy(batch_labels, batch_pred)*100))\n",
    "#                 print('Validation accuracy: %.1f%%' % (valid_accuracy.eval()*100))\n",
    "    #             print('bias ', layer1_biases.eval())\n",
    "    #             print(np.ravel(layer0_weights[0].eval()))\n",
    "    #             print(np.ravel(layer0_weights[1].eval()))\n",
    "\n",
    "        print('Test accuracy: %.1f%%' % (accuracy(testY,test_prediction.eval())*100))       \n",
    "        print('Test ROC AUC: %.1f%%' % (roc_auc_score(testY, test_prediction.eval())*100))\n",
    "        sizes.append(size)\n",
    "        test_accuracies.append(accuracy(testY,test_prediction.eval())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHYy0MQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce246346d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(sizes, test_accuracies)\n",
    "plt.title('Accuracy VS Training set size, original formulation')\n",
    "plt.ylabel('Test Set Accuracy (%)')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "improved_accuracies = [99.6, 99.6, 99.6, 99.6, 99.6, 99.6, 99.6, 99.6, \n",
    "                       99.2, 99.6, 99.6, 98.8, 99.2, 98.4, 99.6, 98.8, \n",
    "                       99.2, 98.8, 98.0, 95.6, 80.4, 69.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce2c321c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(sizes, test_accuracies, label='Original Formulation')\n",
    "plt.scatter(sizes, improved_accuracies, label='Simplified Model')\n",
    "plt.title('Accuracy VS Training set size')\n",
    "plt.ylabel('Test Set Accuracy (%)')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
